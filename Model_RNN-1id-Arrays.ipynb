{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train files loaded\n",
      "1 train files loaded\n",
      "2 train files loaded\n",
      "3 train files loaded\n",
      "4 train files loaded\n",
      "5 train files loaded\n",
      "6 train files loaded\n",
      "7 train files loaded\n",
      "8 train files loaded\n",
      "9 train files loaded\n",
      "10 train files loaded\n",
      "11 train files loaded\n",
      "12 train files loaded\n",
      "13 train files loaded\n",
      "14 train files loaded\n",
      "15 train files loaded\n",
      "16 train files loaded\n",
      "17 train files loaded\n",
      "18 train files loaded\n",
      "19 train files loaded\n",
      "20 train files loaded\n",
      "21 train files loaded\n",
      "22 train files loaded\n",
      "23 train files loaded\n",
      "24 train files loaded\n",
      "25 train files loaded\n",
      "26 train files loaded\n",
      "27 train files loaded\n",
      "28 train files loaded\n",
      "29 train files loaded\n",
      "30 train files loaded\n",
      "31 train files loaded\n",
      "32 train files loaded\n",
      "33 train files loaded\n",
      "34 train files loaded\n",
      "Loaded train data\n",
      "0 test files loaded\n",
      "1 test files loaded\n",
      "2 test files loaded\n",
      "3 test files loaded\n",
      "4 test files loaded\n",
      "5 test files loaded\n",
      "6 test files loaded\n",
      "7 test files loaded\n",
      "8 test files loaded\n",
      "9 test files loaded\n",
      "10 test files loaded\n",
      "11 test files loaded\n",
      "12 test files loaded\n",
      "13 test files loaded\n",
      "14 test files loaded\n",
      "15 test files loaded\n",
      "16 test files loaded\n",
      "Wall time: 6min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "for n,i in enumerate([0,1,2,4,5,7,8,10,11,14,15,16,18,19,20,21,23,25,26,28,29,30,31,32,34,36,37,38,40,43,44,45,47,48,50]):\n",
    "    if n==0:\n",
    "        with h5py.File(f'D:\\Projects\\Goods_prediction\\\\real_data\\Dat9\\data{i}.h5', 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "        train_data=data[:,:400]\n",
    "        train_label=data[:,400:]\n",
    "    else:\n",
    "        with h5py.File(f'D:\\Projects\\Goods_prediction\\\\real_data\\Dat9\\data{i}.h5', 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "        train_data=np.concatenate([train_data,data[:,:400]])\n",
    "        train_label=np.concatenate([train_label,data[:,400:]])\n",
    "    print(f'{n} train files loaded')\n",
    "del data\n",
    "print('Loaded train data')\n",
    "\n",
    "for n,i in enumerate([3,6,9,12,13,17,22,24,27,33,35,39,41,42,46,49,51]):\n",
    "    if n==0:\n",
    "        with h5py.File(f'D:\\Projects\\Goods_prediction\\\\real_data\\Dat9\\data{i}.h5', 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "        val_data=data[:,:400]\n",
    "        val_label=data[:,400:]\n",
    "    else:\n",
    "        with h5py.File(f'D:\\Projects\\Goods_prediction\\\\real_data\\Dat9\\data{i}.h5', 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "        val_data=np.concatenate([val_data,data[:,:400]])\n",
    "        val_label=np.concatenate([val_label,data[:,400:]])\n",
    "    print(f'{n} test files loaded')\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# import pickle\n",
    "\n",
    "# with open(r\"D:\\Projects\\Goods_prediction\\train_data.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(train_data,f, protocol=4)\n",
    "# with open(r\"D:\\Projects\\Goods_prediction\\train_target.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(train_target,f, protocol=4) \n",
    "# with open(r\"D:\\Projects\\Goods_prediction\\test_data.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(test_data,f, protocol=4)\n",
    "# with open(r\"D:\\Projects\\Goods_prediction\\test_target.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(test_target,f, protocol=4) \n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1007 22:30:03.019440 15560 deprecation_wrapper.py:119] From C:\\Users\\ToschevikovG.EUROOPT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W1007 22:30:03.019440 15560 deprecation_wrapper.py:119] From C:\\Users\\ToschevikovG.EUROOPT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1007 22:30:03.029454 15560 deprecation_wrapper.py:119] From C:\\Users\\ToschevikovG.EUROOPT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1007 22:30:03.099398 15560 deprecation_wrapper.py:119] From C:\\Users\\ToschevikovG.EUROOPT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1007 22:30:03.099398 15560 deprecation_wrapper.py:119] From C:\\Users\\ToschevikovG.EUROOPT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1007 22:30:03.129413 15560 deprecation_wrapper.py:119] From C:\\Users\\ToschevikovG.EUROOPT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1007 22:30:03.219360 15560 deprecation.py:323] From C:\\Users\\ToschevikovG.EUROOPT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5674557 samples, validate on 2705070 samples\n",
      "Epoch 1/25\n",
      "5674557/5674557 [==============================] - 3161s 557us/step - loss: 16.7409 - val_loss: 14.7284\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 14.72843, saving model to D:\\Projects\\Goods_prediction\\real_data\\model_rnn_1l.hdf5\n",
      "Epoch 2/25\n",
      "5674557/5674557 [==============================] - 3162s 557us/step - loss: 14.7678 - val_loss: 14.6044\n",
      "\n",
      "Epoch 00002: val_loss improved from 14.72843 to 14.60435, saving model to D:\\Projects\\Goods_prediction\\real_data\\model_rnn_1l.hdf5\n",
      "2705070/2705070 [==============================] - 406s 150us/step\n",
      "0.3 limit top5_accuracy: 0.20566633808380708 all_accuracy 1.0 mean_goods_number 35.540956426266234\n",
      "0.4 limit top5_accuracy: 0.4062816340850344 all_accuracy 0.9446868144377253 mean_goods_number 9.440135417766621\n",
      "0.5 limit top5_accuracy: 0.6036085559358126 all_accuracy 0.5651103829673669 mean_goods_number 4.1842466876557785\n",
      "Epoch 3/25\n",
      "5674557/5674557 [==============================] - 3161s 557us/step - loss: 14.6088 - val_loss: 14.6201\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 14.60435\n",
      "Epoch 4/25\n",
      "5674557/5674557 [==============================] - 3165s 558us/step - loss: 14.5132 - val_loss: 14.6556\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 14.60435\n",
      "Epoch 5/25\n",
      "5674557/5674557 [==============================] - 3165s 558us/step - loss: 14.4406 - val_loss: 14.6963\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 14.60435\n",
      "2705070/2705070 [==============================] - 406s 150us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense,Dropout,Input,BatchNormalization,LSTM\n",
    "from keras.layers import GRU,Bidirectional,GlobalMaxPooling1D,Conv1D,Flatten,Embedding,GlobalAveragePooling1D\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint,Callback,ReduceLROnPlateau,EarlyStopping\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from joblib import Parallel, delayed\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "K.set_session(tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16)))\n",
    "K.clear_session()\n",
    "def accur(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        try:\n",
    "            score.append(sum([np.any(i==itog_true[:]) for i in itog_pred])/len(itog_pred))\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            score.append(sum([np.any(i==itog_true) for i in itog_pred])/len(itog_pred))\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return np.mean(score)\n",
    "def accur2(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    score_null=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        if itog_pred.size>0:\n",
    "            try:\n",
    "                score.append(int(sum([np.any(i==itog_true[:]) for i in itog_pred])>0))\n",
    "            except IndexError:\n",
    "                score.append(int(sum([np.any(i==itog_true) for i in itog_pred])>0))\n",
    "            except TypeError:\n",
    "                pass\n",
    "        else:\n",
    "            score_null.append(int(itog_true.size==0))\n",
    "    return np.mean(score),len(score)/(len(score)+len(score_null))\n",
    "def top_category(y_true, y_pred):\n",
    "    result=[]\n",
    "    result=Parallel(n_jobs=-2)(delayed(accur2)(y_true, y_pred,l) for l in [0.1,0.3,0.5])\n",
    "#    for l in [0.1,0.29]:\n",
    "#        result.append(accur2(y_true,y_pred,limit=l))\n",
    "    return result\n",
    "def my_accur(y_true, y_pred):\n",
    "    result=[]\n",
    "    for l in np.arange(0.3,0.95,0.01):\n",
    "        prediction=K.cast(K.greater(y_pred,l),K.floatx())\n",
    "        result.append(K.mean(K.equal(y_true,prediction)))\n",
    " #   print(np.argmax(result)*0.01+0.1)\n",
    "    return K.max(result)\n",
    "from sklearn.metrics import fbeta_score\n",
    "def fbeta_predict(prediction,labels):\n",
    "    score = []\n",
    "    candidates = np.arange(0, 0.5, 0.01)\n",
    "    for th in candidates:\n",
    "        yp = (prediction > th).astype(int)\n",
    "        score.append(fbeta_score(y_pred=yp, y_true=labels, beta=2, average=\"samples\"))\n",
    "    score = np.array(score)\n",
    "    pm = score.argmax()\n",
    "    best_th, best_score = candidates[pm], score[pm]\n",
    "    print(f\"Best limit {best_th}\")\n",
    "    return best_score\n",
    "gamma = 2.0\n",
    "epsilon = K.epsilon()\n",
    "def focal_loss(y_true, y_pred):\n",
    "    pt = y_pred * y_true + (1-y_pred) * (1-y_true)\n",
    "    pt = K.clip(pt, epsilon, 1-epsilon)\n",
    "    CE = -K.log(pt)\n",
    "    FL = K.pow(1-pt, gamma) * CE\n",
    "    loss = K.sum(FL, axis=1)\n",
    "    return loss\n",
    "def accur2(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    score_null=[]\n",
    "    itog_num=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        if itog_pred.size>0:\n",
    "            try:\n",
    "                score.append(sum([np.any(i==itog_true[:]) for i in itog_pred])/len(itog_pred))\n",
    "                itog_num.append(len(itog_pred))\n",
    "            except IndexError:\n",
    "                score.append(sum([np.any(i==itog_true) for i in itog_pred])/len(itog_pred))\n",
    "                itog_num.append(len(itog_pred))\n",
    "            except TypeError:\n",
    "                pass\n",
    "        else:\n",
    "            score_null.append(int(itog_true.size==0))\n",
    "    return np.mean(score),len(score)/(len(score)+len(score_null)),np.mean(itog_num)\n",
    "def top_category(y_true, y_pred):\n",
    "    result=[]\n",
    " #   result=Parallel(n_jobs=-2)(delayed(accur2)(y_true, y_pred,l) for l in [0.3,0.4,0.5])\n",
    "    for l in [0.3,0.4,0.5]:\n",
    "        result.append(accur2(y_true,y_pred,limit=l))\n",
    "    return result\n",
    "\n",
    "# train_data=norm.transform(train[df_train_columns])\n",
    "# train_label=np.asarray(train_target)\n",
    "# val_data=norm.transform(test[df_train_columns])\n",
    "# val_label=np.asarray(test_target)\n",
    "#train=train_data.copy()\n",
    "#train_data=train[ind]\n",
    "inp=Input(shape=(train_data.shape[1],))\n",
    "emb=Embedding(5656,70)(inp)\n",
    "#x=Bidirectional(LSTM(8,return_sequences=True))(emb)\n",
    "#x=Bidirectional(LSTM(8,return_sequences=True))(x)\n",
    "#x=Attention(train_data.shape[1])(x)\n",
    "x=Flatten()(emb)\n",
    "\n",
    "x=Dense(1925,activation='sigmoid')(x)\n",
    "model=Model(inputs=inp,outputs=x)\n",
    "class Metr(Callback):\n",
    "    def __init__(self,N):\n",
    "        self.N=N\n",
    "#        self.epoch=1\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        if epoch%self.N==1:\n",
    "            val_pr=np.squeeze(model.predict([val_data], batch_size=2024, verbose=1))\n",
    "            result=top_category(val_label,val_pr)\n",
    "            print('0.3 limit top5_accuracy: {} all_accuracy {} mean_goods_number {}'.format(result[0][0],result[0][1],result[0][2]))\n",
    "            print('0.4 limit top5_accuracy: {} all_accuracy {} mean_goods_number {}'.format(result[1][0],result[1][1],result[1][2]))\n",
    "            print('0.5 limit top5_accuracy: {} all_accuracy {} mean_goods_number {}'.format(result[2][0],result[2][1],result[2][2]))\n",
    "metr=Metr(10)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=7, min_lr=1e-6, mode='auto', verbose=1)\n",
    "earlstop=EarlyStopping(patience=3)\n",
    "checkpointer = ModelCheckpoint(filepath='D:\\Projects\\Goods_prediction\\\\real_data\\\\model_rnn_1l.hdf5', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer=adam(), loss=focal_loss)\n",
    "model.fit(train_data,train_label,batch_size=8000,epochs=25,validation_data=(val_data,val_label),verbose=1,callbacks=[earlstop,checkpointer,metr])\n",
    "model.load_weights('D:\\Projects\\Goods_prediction\\\\real_data\\model_rnn_1l.hdf5')\n",
    "model.save(\"model_rnn_1l.h5\")\n",
    "val_pr=np.squeeze(model.predict([val_data], batch_size=1024, verbose=1))\n",
    "#0.6830883802939437, 0.9055531263664189\n",
    "#0.79862720237835, 0.572865057476875\n",
    "#0.1268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.models import load_model\n",
    "model=load_model('D:\\Projects\\Goods_prediction\\\\real_data\\\\model_rnn_mDense_2l.hdf5')\n",
    "val_data=test_data\n",
    "val_label=test_target\n",
    "#val_pr=np.squeeze(model.predict([val_data], batch_size=1024, verbose=0))\n",
    "goods=pd.read_csv('D:\\Projects\\Goods_prediction\\\\real_data\\goods2id.csv',sep=';',names=['GoodsGroup3Id','GoodsGroup3name'])\n",
    "goods=goods.reset_index(drop=False)\n",
    "goods=goods.rename(columns={'index':'ind'})\n",
    "goods=goods['GoodsGroup3name'].values\n",
    "def category_name(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    itog_pred=[]\n",
    "    itog_true=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "#        print(f'pred {[goods[i] for i in [np.squeeze((pred>limit).nonzero()[0])]]}')\n",
    "#        print(f'true {[goods[i] for i in [np.squeeze(np.argwhere(true==1))]]}')\n",
    "\n",
    "        itog_pred.append(np.squeeze([goods[i] for i in [np.squeeze((pred>limit).nonzero()[0])]]))\n",
    "        if len(np.argwhere(true==1))==1:\n",
    "            itog_true.append(np.squeeze([[goods[i],pred[i]] for i in [np.squeeze(np.argwhere(true==1))]]))\n",
    "        else:\n",
    "            itog_true.append(np.squeeze([list(zip(goods[i],pred[i])) for i in [np.squeeze(np.argwhere(true==1))]]))            \n",
    "    return itog_true,itog_pred\n",
    "true_label,predict_label=category_name(val_label,val_pr,0.5)\n",
    "pd.DataFrame({'true_label':true_label,'predict_label':predict_label}).to_csv('resultrnn2id.csv',sep=';',encoding='cp1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6411636542956115, 0.7387811576277227, 4.278121131750185), (0.6529116224042888, 0.7056164408060762, 4.118380515278466), (0.6645067154495161, 0.6714930687428788, 3.9675537830353225)]\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ind=np.random.randint(0,len(train_data),size=1000000)\n",
    "#val_data=test_data\n",
    "from keras.models import load_model\n",
    "#model=load_model('D:\\Projects\\Goods_prediction\\\\real_data\\\\model_rnn_mDense_2l.hdf5')\n",
    "#model.load_weights('D:\\Projects\\Goods_prediction\\\\real_data\\model_weights_rnn.hdf5')\n",
    "#val_pr=np.squeeze(model.predict([val_data], batch_size=2048, verbose=0))\n",
    "def test():\n",
    "    ttest=pd.read_hdf('D:\\Projects\\Goods_prediction\\\\real_data\\\\Dat3\\\\test.h5','data')['goods'][0]\n",
    "#    ttest=ttest[df_train_columns].values\n",
    "#    norm.transform(ttest,copy=False)\n",
    "    pred=np.squeeze(model.predict([[ttest]], batch_size=1024, verbose=1))\n",
    "    return np.squeeze([goods[i] for i in [np.squeeze((pred>0.4).nonzero()[0])]]) \n",
    "\n",
    "def accur2(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    score_null=[]\n",
    "    itog_num=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        if itog_pred.size>0:\n",
    "            try:\n",
    "                score.append(sum([np.any(i==itog_true[:]) for i in itog_pred])/len(itog_pred))\n",
    "                itog_num.append(len(itog_pred))\n",
    "            except IndexError:\n",
    "                score.append(sum([np.any(i==itog_true) for i in itog_pred])/len(itog_pred))\n",
    "                itog_num.append(len(itog_pred))\n",
    "            except TypeError:\n",
    "                pass\n",
    "        else:\n",
    "            score_null.append(int(itog_true.size==0))\n",
    "    return np.mean(score),len(score)/(len(score)+len(score_null)),np.mean(itog_num)\n",
    "def top_category(y_true, y_pred):\n",
    "    result=[]\n",
    "#    result=Parallel(n_jobs=3)(delayed(accur2)(y_true, y_pred,l) for l in [0.3,0.4,0.5])\n",
    "    for l in [0.52,0.54,0.56]:\n",
    "        result.append(accur2(y_true,y_pred,limit=l))\n",
    "    return result\n",
    "print(top_category(val_label,val_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top_category(val_label,val_pr)\n",
    "isinstance(292,int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind</th>\n",
       "      <th>GoodsGroup3Id</th>\n",
       "      <th>GoodsGroup3name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>623</td>\n",
       "      <td>2563</td>\n",
       "      <td>Корма для грызунов</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ind  GoodsGroup3Id     GoodsGroup3name\n",
       "623  623           2563  Корма для грызунов"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goods=pd.read_csv('D:\\Projects\\Goods_prediction\\\\real_data\\goods2id.csv',sep=';',names=['GoodsGroup3Id','GoodsGroup3name'])\n",
    "goods=goods.reset_index(drop=False)\n",
    "goods=goods.rename(columns={'index':'ind'})\n",
    "goods\n",
    "goods[goods['ind']==623]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ToschevikovG.EUROOPT\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('333', 0.547301173210144),\n",
       " ('623', 0.5419012308120728),\n",
       " ('187', 0.5062810182571411),\n",
       " ('368', 0.4683801531791687),\n",
       " ('49', 0.4316135048866272),\n",
       " ('580', 0.41029298305511475),\n",
       " ('371', 0.40346166491508484),\n",
       " ('276', 0.40021929144859314),\n",
       " ('512', 0.3969462215900421),\n",
       " ('289', 0.3889583647251129)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top_category(test_target,val_pr)\n",
    "mm.most_similar('52')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense,Dropout,Input,BatchNormalization,LSTM\n",
    "from keras.layers import GRU,Bidirectional,GlobalMaxPooling1D,Conv1D,Flatten,Embedding,GlobalAveragePooling1D\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint,Callback,ReduceLROnPlateau,EarlyStopping\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from joblib import Parallel, delayed\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "K.set_session(tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16)))\n",
    "K.clear_session()\n",
    "def accur(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        try:\n",
    "            score.append(sum([np.any(i==itog_true[:]) for i in itog_pred])/len(itog_pred))\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            score.append(sum([np.any(i==itog_true) for i in itog_pred])/len(itog_pred))\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return np.mean(score)\n",
    "def accur2(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    score_null=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        if itog_pred.size>0:\n",
    "            try:\n",
    "                score.append(int(sum([np.any(i==itog_true[:]) for i in itog_pred])>0))\n",
    "            except IndexError:\n",
    "                score.append(int(sum([np.any(i==itog_true) for i in itog_pred])>0))\n",
    "            except TypeError:\n",
    "                pass\n",
    "        else:\n",
    "            score_null.append(int(itog_true.size==0))\n",
    "    return np.mean(score),len(score)/(len(score)+len(score_null))\n",
    "def top_category(y_true, y_pred):\n",
    "    result=[]\n",
    "    result=Parallel(n_jobs=-2)(delayed(accur2)(y_true, y_pred,l) for l in [0.1,0.3,0.5])\n",
    "#    for l in [0.1,0.29]:\n",
    "#        result.append(accur2(y_true,y_pred,limit=l))\n",
    "    return result\n",
    "def my_accur(y_true, y_pred):\n",
    "    result=[]\n",
    "    for l in np.arange(0.3,0.95,0.01):\n",
    "        prediction=K.cast(K.greater(y_pred,l),K.floatx())\n",
    "        result.append(K.mean(K.equal(y_true,prediction)))\n",
    " #   print(np.argmax(result)*0.01+0.1)\n",
    "    return K.max(result)\n",
    "from sklearn.metrics import fbeta_score\n",
    "def fbeta_predict(prediction,labels):\n",
    "    score = []\n",
    "    candidates = np.arange(0, 0.5, 0.01)\n",
    "    for th in candidates:\n",
    "        yp = (prediction > th).astype(int)\n",
    "        score.append(fbeta_score(y_pred=yp, y_true=labels, beta=2, average=\"samples\"))\n",
    "    score = np.array(score)\n",
    "    pm = score.argmax()\n",
    "    best_th, best_score = candidates[pm], score[pm]\n",
    "    print(f\"Best limit {best_th}\")\n",
    "    return best_score\n",
    "gamma = 2.0\n",
    "epsilon = K.epsilon()\n",
    "def focal_loss(y_true, y_pred):\n",
    "    pt = y_pred * y_true + (1-y_pred) * (1-y_true)\n",
    "    pt = K.clip(pt, epsilon, 1-epsilon)\n",
    "    CE = -K.log(pt)\n",
    "    FL = K.pow(1-pt, gamma) * CE\n",
    "    loss = K.sum(FL, axis=1)\n",
    "    return loss\n",
    "def accur2(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    score_null=[]\n",
    "    itog_num=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        if itog_pred.size>0:\n",
    "            try:\n",
    "                score.append(sum([np.any(i==itog_true[:]) for i in itog_pred])/len(itog_pred))\n",
    "                itog_num.append(len(itog_pred))\n",
    "            except IndexError:\n",
    "                score.append(sum([np.any(i==itog_true) for i in itog_pred])/len(itog_pred))\n",
    "                itog_num.append(len(itog_pred))\n",
    "            except TypeError:\n",
    "                pass\n",
    "        else:\n",
    "            score_null.append(int(itog_true.size==0))\n",
    "    return np.mean(score),len(score)/(len(score)+len(score_null)),np.mean(itog_num)\n",
    "def top_category(y_true, y_pred):\n",
    "    result=[]\n",
    " #   result=Parallel(n_jobs=-2)(delayed(accur2)(y_true, y_pred,l) for l in [0.3,0.4,0.5])\n",
    "    for l in [0.3,0.4,0.5]:\n",
    "        result.append(accur2(y_true,y_pred,limit=l))\n",
    "    return result\n",
    "val_pr=np.zeros(len(test_data))\n",
    "# train_data=norm.transform(train[df_train_columns])\n",
    "# train_label=np.asarray(train_target)\n",
    "# val_data=norm.transform(test[df_train_columns])\n",
    "# val_label=np.asarray(test_target)\n",
    "#train=train_data.copy()\n",
    "#train_data=train[ind]\n",
    "train_label=train_target\n",
    "val_data=test_data\n",
    "val_label=test_target\n",
    "inp=Input(shape=(train_data.shape[1],))\n",
    "emb=Embedding(5656,50)(inp)\n",
    "#x=Bidirectional(LSTM(8,return_sequences=True))(emb)\n",
    "#x=Bidirectional(LSTM(8,return_sequences=True))(x)\n",
    "#x=Attention(train_data.shape[1])(x)\n",
    "x=Flatten()(emb)\n",
    "\n",
    "x=Dense(788,activation='sigmoid')(x)\n",
    "model=Model(inputs=inp,outputs=x)\n",
    "class Metr(Callback):\n",
    "    def __init__(self,N):\n",
    "        self.N=N\n",
    "#        self.epoch=1\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        if epoch%self.N==0:\n",
    "            val_pr=np.squeeze(model.predict([val_data], batch_size=1024, verbose=1))\n",
    "            result=top_category(val_label,val_pr)\n",
    "            print('0.3 limit top5_accuracy: {} all_accuracy {} mean_goods_number {}'.format(result[0][0],result[0][1],result[0][2]))\n",
    "            print('0.4 limit top5_accuracy: {} all_accuracy {} mean_goods_number {}'.format(result[1][0],result[1][1],result[1][2]))\n",
    "            print('0.5 limit top5_accuracy: {} all_accuracy {} mean_goods_number {}'.format(result[2][0],result[2][1],result[2][2]))\n",
    "metr=Metr(10)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=7, min_lr=1e-6, mode='auto', verbose=1)\n",
    "earlstop=EarlyStopping(patience=3)\n",
    "checkpointer = ModelCheckpoint(filepath='D:\\Projects\\Goods_prediction\\\\real_data\\\\model_50rnn_1l.hdf5', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer=adam(), loss='binary_crossentropy')\n",
    "model.fit(train_data,train_label,batch_size=8000,epochs=25,validation_data=(val_data,val_label),verbose=1,callbacks=[earlstop,checkpointer,metr])\n",
    "model.load_weights('D:\\Projects\\Goods_prediction\\\\real_data\\model_50rnn_1l.hdf5')\n",
    "model.save(\"model_50rnn_1l.h5\")\n",
    "val_pr=np.squeeze(model.predict([val_data], batch_size=1024, verbose=1))\n",
    "#0.6830883802939437, 0.9055531263664189\n",
    "#0.79862720237835, 0.572865057476875\n",
    "#0.1268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense,Dropout,Input,BatchNormalization,LSTM\n",
    "from keras.layers import GRU,Bidirectional,GlobalMaxPooling1D,Conv1D,Flatten,Embedding,GlobalAveragePooling1D\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint,Callback,ReduceLROnPlateau,EarlyStopping\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from joblib import Parallel, delayed\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "K.set_session(tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16)))\n",
    "K.clear_session()\n",
    "def accur(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        try:\n",
    "            score.append(sum([np.any(i==itog_true[:]) for i in itog_pred])/len(itog_pred))\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            score.append(sum([np.any(i==itog_true) for i in itog_pred])/len(itog_pred))\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return np.mean(score)\n",
    "def accur2(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    score_null=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        if itog_pred.size>0:\n",
    "            try:\n",
    "                score.append(int(sum([np.any(i==itog_true[:]) for i in itog_pred])>0))\n",
    "            except IndexError:\n",
    "                score.append(int(sum([np.any(i==itog_true) for i in itog_pred])>0))\n",
    "            except TypeError:\n",
    "                pass\n",
    "        else:\n",
    "            score_null.append(int(itog_true.size==0))\n",
    "    return np.mean(score),len(score)/(len(score)+len(score_null))\n",
    "def top_category(y_true, y_pred):\n",
    "    result=[]\n",
    "    result=Parallel(n_jobs=-2)(delayed(accur2)(y_true, y_pred,l) for l in [0.1,0.3,0.5])\n",
    "#    for l in [0.1,0.29]:\n",
    "#        result.append(accur2(y_true,y_pred,limit=l))\n",
    "    return result\n",
    "def my_accur(y_true, y_pred):\n",
    "    result=[]\n",
    "    for l in np.arange(0.3,0.95,0.01):\n",
    "        prediction=K.cast(K.greater(y_pred,l),K.floatx())\n",
    "        result.append(K.mean(K.equal(y_true,prediction)))\n",
    " #   print(np.argmax(result)*0.01+0.1)\n",
    "    return K.max(result)\n",
    "from sklearn.metrics import fbeta_score\n",
    "def fbeta_predict(prediction,labels):\n",
    "    score = []\n",
    "    candidates = np.arange(0, 0.5, 0.01)\n",
    "    for th in candidates:\n",
    "        yp = (prediction > th).astype(int)\n",
    "        score.append(fbeta_score(y_pred=yp, y_true=labels, beta=2, average=\"samples\"))\n",
    "    score = np.array(score)\n",
    "    pm = score.argmax()\n",
    "    best_th, best_score = candidates[pm], score[pm]\n",
    "    print(f\"Best limit {best_th}\")\n",
    "    return best_score\n",
    "gamma = 2.0\n",
    "epsilon = K.epsilon()\n",
    "def focal_loss(y_true, y_pred):\n",
    "    pt = y_pred * y_true + (1-y_pred) * (1-y_true)\n",
    "    pt = K.clip(pt, epsilon, 1-epsilon)\n",
    "    CE = -K.log(pt)\n",
    "    FL = K.pow(1-pt, gamma) * CE\n",
    "    loss = K.sum(FL, axis=1)\n",
    "    return loss\n",
    "def accur2(y_true, y_pred,limit):\n",
    "#     itog_pred=K.greater(pred>limit)\n",
    "#     itog_true=K.equal(y_true,1)\n",
    "    score=[]\n",
    "    score_null=[]\n",
    "    itog_num=[]\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        itog_pred=np.squeeze((pred>limit).nonzero()[0])\n",
    "        itog_true=np.squeeze(np.argwhere(true==1))\n",
    "        if itog_pred.size>0:\n",
    "            try:\n",
    "                score.append(sum([np.any(i==itog_true[:]) for i in itog_pred])/len(itog_pred))\n",
    "                itog_num.append(len(itog_pred))\n",
    "            except IndexError:\n",
    "                score.append(sum([np.any(i==itog_true) for i in itog_pred])/len(itog_pred))\n",
    "                itog_num.append(len(itog_pred))\n",
    "            except TypeError:\n",
    "                pass\n",
    "        else:\n",
    "            score_null.append(int(itog_true.size==0))\n",
    "    return np.mean(score),len(score)/(len(score)+len(score_null)),np.mean(itog_num)\n",
    "def top_category(y_true, y_pred):\n",
    "    result=[]\n",
    " #   result=Parallel(n_jobs=-2)(delayed(accur2)(y_true, y_pred,l) for l in [0.3,0.4,0.5])\n",
    "    for l in [0.3,0.4,0.5]:\n",
    "        result.append(accur2(y_true,y_pred,limit=l))\n",
    "    return result\n",
    "val_pr=np.zeros(len(test_data))\n",
    "# train_data=norm.transform(train[df_train_columns])\n",
    "# train_label=np.asarray(train_target)\n",
    "# val_data=norm.transform(test[df_train_columns])\n",
    "# val_label=np.asarray(test_target)\n",
    "#train=train_data.copy()\n",
    "#train_data=train[ind]\n",
    "train_label=train_target\n",
    "val_data=test_data\n",
    "val_label=test_target\n",
    "inp=Input(shape=(train_data.shape[1],))\n",
    "emb=Embedding(5656,90)(inp)\n",
    "#x=Bidirectional(LSTM(8,return_sequences=True))(emb)\n",
    "#x=Bidirectional(LSTM(8,return_sequences=True))(x)\n",
    "#x=Attention(train_data.shape[1])(x)\n",
    "x=Flatten()(emb)\n",
    "\n",
    "x=Dense(788,activation='sigmoid')(x)\n",
    "model=Model(inputs=inp,outputs=x)\n",
    "class Metr(Callback):\n",
    "    def __init__(self,N):\n",
    "        self.N=N\n",
    "#        self.epoch=1\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        if epoch%self.N==0:\n",
    "            val_pr=np.squeeze(model.predict([val_data], batch_size=1024, verbose=1))\n",
    "            result=top_category(val_label,val_pr)\n",
    "            print('0.3 limit top5_accuracy: {} all_accuracy {} mean_goods_number {}'.format(result[0][0],result[0][1],result[0][2]))\n",
    "            print('0.4 limit top5_accuracy: {} all_accuracy {} mean_goods_number {}'.format(result[1][0],result[1][1],result[1][2]))\n",
    "            print('0.5 limit top5_accuracy: {} all_accuracy {} mean_goods_number {}'.format(result[2][0],result[2][1],result[2][2]))\n",
    "metr=Metr(10)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=7, min_lr=1e-6, mode='auto', verbose=1)\n",
    "earlstop=EarlyStopping(patience=3)\n",
    "checkpointer = ModelCheckpoint(filepath='D:\\Projects\\Goods_prediction\\\\real_data\\\\model_90rnn_1l.hdf5', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer=adam(), loss='binary_crossentropy')\n",
    "model.fit(train_data,train_label,batch_size=8000,epochs=25,validation_data=(val_data,val_label),verbose=1,callbacks=[earlstop,checkpointer,metr])\n",
    "model.load_weights('D:\\Projects\\Goods_prediction\\\\real_data\\model_90rnn_1l.hdf5')\n",
    "model.save(\"model_90rnn_1l.h5\")\n",
    "val_pr=np.squeeze(model.predict([val_data], batch_size=1024, verbose=1))\n",
    "#0.6830883802939437, 0.9055531263664189\n",
    "#0.79862720237835, 0.572865057476875\n",
    "#0.1268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 13, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7d32b8409a39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'resultrnn2id.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nrows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 13, saw 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a=pd.read_csv('resultrnn2id.csv')\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
